{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNZL5vTcjELvy3RJ9wWw6i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcos-pd/IA_Prompting_CoderHouse/blob/main/Entrega_Final_CoderHouse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generación de contenidos de difusión a partir de informes técnicos**\n",
        "*Marcos Ferrrario*"
      ],
      "metadata": {
        "id": "HgfKpUh6basZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*La solución que vamos a construir permite cargar un informe en pdf y mediante herramientas y librerías en Python extraer el texto que contiene. Ese texto se guardará en en un archivo JSON y se pasará a un modelo LLM para que, mediante un prompt optimizado, el modelo devuelva un texto con engagement que pueda usarse para su difusión mediante RS, email y publicación en web institucional.\n",
        "\n",
        "La solución se optimiza haciendo que el mismo resultado del LLM, genere un prompt coherente para pasarlo a Dall-E y que éste genere una imágen para ilustrar la publicación.*\n"
      ],
      "metadata": {
        "id": "Fcdy_mA-b_7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *EXTRACIÓN DE TEXTO desde PDF*"
      ],
      "metadata": {
        "id": "LPocFOJmcvN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para la extracción de PDFs, las bibliotecas más comunes y robustas en Python son PyPDF2 (para PDFs basados en texto) y pdfplumber (que maneja mejor la estructura y tablas, y es más potente).\n",
        "\n",
        "No es nuestro caso, pero para resolver el problema de pdfs escaneados, hay que usar una herramienta de OCR, donde PyTesseract es una buena opción."
      ],
      "metadata": {
        "id": "MASfBeCD3jfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar bibliotecas necesarias (ejecutar solo una vez por sesión)\n",
        "!pip install PyPDF2\n",
        "!pip install pdfplumber\n",
        "!pip install google-colab\n",
        "\n",
        "from google.colab import files\n",
        "import pdfplumber\n",
        "import re # Para futuras extracciones numéricas"
      ],
      "metadata": {
        "id": "ev5GZpxiEKDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subir el archivo PDF\n",
        "print(\"Por favor, selecciona y sube tu archivo PDF (solo uno):\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Validamos la subida del archivo. Funciona mentras subas un solo archivo.\n",
        "if uploaded:\n",
        "    pdf_file_path = list(uploaded.keys())[0]\n",
        "    print(f\"Archivo '{pdf_file_path}' subido exitosamente.\")\n",
        "else:\n",
        "    pdf_file_path = None\n",
        "    print(\"No se ha subido ningún archivo.\")"
      ],
      "metadata": {
        "id": "yZpgjBXaekWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraemos el texto del PDF\n",
        "def extract_info_from_pdf_pdfplumber(pdf_path):\n",
        "    full_text = \"\"\n",
        "    extracted_tables = []\n",
        "\n",
        "    if not pdf_path:\n",
        "        print(\"Error: No se proporcionó una ruta de archivo PDF válida.\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                # Extraer texto de la página\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    full_text += page_text + \"\\n\\n\"\n",
        "\n",
        "                # Extraer tablas de la página\n",
        "                tables = page.extract_tables()\n",
        "                if tables:\n",
        "                    extracted_tables.extend(tables)\n",
        "    except Exception as e:\n",
        "        print(f\"Error al procesar el PDF '{pdf_path}': {e}\")\n",
        "        return None, None\n",
        "\n",
        "    return full_text, extracted_tables"
      ],
      "metadata": {
        "id": "tcgPOZm9EMU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando la extracción\n",
        "if 'pdf_file_path' in locals() and pdf_file_path:\n",
        "    print(f\"\\nIntentando extraer información de: {pdf_file_path}\")\n",
        "    extracted_text_pdfplumber, extracted_tables_pdfplumber = extract_info_from_pdf_pdfplumber(pdf_file_path)\n",
        "\n",
        "    if extracted_text_pdfplumber is not None:\n",
        "        print(\"\\n--- Texto extraído con pdfplumber (primeros 1000 caracteres) ---\")\n",
        "        print(extracted_text_pdfplumber[:1000])\n",
        "        print(\"\\n--- Fin del fragmento de texto ---\")\n",
        "\n",
        "        if len(extracted_text_pdfplumber) > 1000:\n",
        "            print(f\"\\n(El texto completo tiene {len(extracted_text_pdfplumber)} caracteres. Se mostraron los primeros 1000.)\")\n",
        "\n",
        "        if extracted_tables_pdfplumber:\n",
        "            print(f\"\\n--- Se encontraron {len(extracted_tables_pdfplumber)} tabla(s) ---\")\n",
        "            for i, table in enumerate(extracted_tables_pdfplumber):\n",
        "                print(f\"\\nTabla {i+1} (primeras 3 filas):\")\n",
        "                for row_idx, row in enumerate(table):\n",
        "                    print(row)\n",
        "                    if row_idx >= 2:\n",
        "                        break\n",
        "                if i >= 0: # Para mostrar solo la primera tabla.\n",
        "                    break\n",
        "        else:\n",
        "            print(\"\\n--- No se encontraron tablas con pdfplumber. ---\")\n",
        "    else:\n",
        "        print(\"\\n--- La extracción de PDF falló o no se pudo obtener texto/tablas. ---\")\n",
        "else:\n",
        "    print(\"\\nError: La ruta del archivo PDF no está disponible. Asegurate de haber subido el archivo  y de que la variable 'pdf_file_path' se haya creado correctamente.\")"
      ],
      "metadata": {
        "id": "HrsqiEFwEOpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *PROCESAMIENTO DEL TEXTO EXTRAIDO a través del modelo OpenAI gpt 4o*"
      ],
      "metadata": {
        "id": "YNtGsAQUlILN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import json\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Carga la clave de API desde \"los secretos\" de Colab\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Inicializa el cliente de OpenAI\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Prueba una pequeña solicitud para verificar la conexión\n",
        "try:\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\", #\"gpt-3.5-turbo\"\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Hola, ¿estás funcionando?\"}]\n",
        "    )\n",
        "    print(\"Conexión a OpenAI exitosa:\", response.choices[0].message.content)\n",
        "except Exception as e:\n",
        "    print(f\"Error al conectar con OpenAI: {e}\")\n",
        "    print(\"Asegúrate de que tu clave de API esté configurada correctamente en los secretos de Colab y que tengas saldo disponible.\")"
      ],
      "metadata": {
        "id": "xVLeqxDtHjt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora sí. Vamos a pasarle a ChatGPT un prompt **usando la técnica de One Shot Prompting** para darle contexto y especificarle al modelo que genere un resultado que optimice el uso de recursos y minimice los costos.\n",
        "\n",
        "Este prompt le pedirá al sistema que genere un texto para publicar y un prompt para que Dall-E genere una imágen ilustrativa para la publicación, por ejemplo, en Linkedin.\n",
        "\n",
        "**Este método nos permite ahorrar costos ya que realizamos una sola llamada a la API en lugar de dos y aumentar la coherencia, ya que el mismo modelo que entendió y resumió el texto es el que sugiere la imagen, asegurando que el concepto visual esté perfectamente alineado con el textual**.\n",
        "\n",
        "El prompt le pedirá a la API que genere un JSON con dos claves: una para el texto y otra para el prompt de la imagen.\n",
        "\n",
        "Luego usará esas dos salidas para generar un archivo .txt con los textos para la publicación, que se va a guardar en una carpeta en Google Drive y llamará a Dall-E para generar la imágen a partir de la segunda parte de la salida."
      ],
      "metadata": {
        "id": "3muSRrcXWA3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'extracted_text_pdfplumber' in locals() and extracted_text_pdfplumber:\n",
        "    prompt_messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Eres economista con muy buen conocimento de la macroeconomía argentina y un fuerte expertice en el mercado automotor. Tu diferencial como analista es tu capacidad para entender las implicancias de las novedades macroeconómicas en el negocio automotriz, desde la industria (fábricas, autopartistas o importadores) hasta el retail minorista (la venta de vehículos nuevos y usados al consumidor final). En tu rol, tienes el objetivo de analizar los informes técnicos que se te proveeran y a partir de ellos generar contenido para los canales de difusión de la empresa que los promueve. Ese contenido está orientado a Redes Sociales como Linkedin y a textos con engadgement para difundir los informes por email o en el sitio web de la empresa. Para generar esos contenidos, es importante que mantengas ciertas cualidades del lenguaje humano a través de las siguientes premisas: 1) Manteniendo la estructura general del texto y el rigor técnico, agregando emociones sutiles, expresiones humanas y lenguaje conversacional. 2) Redactar el texto con frases de ritmo variado, mezclando oraciones cortas con oraciones largas. 3) Sustituye palabras genéricas por otras más específicas que contengan carga emocional. 4) Agrega conectores suaves y naturales como “además”, “y es que”, etc. En la medida de lo posible usa ejemplos, analogías o detalles que hagan más cercano el texto al lector. 5) Evita estructuras gramaticales repetitivas, frases simétricas o lenguaje neutro. Finalmente, estructura tu respuesta únicamente como un objeto JSON válido con las siguientes dos claves: 'texto_linkedin' (que contendrá el texto generado para la publicación) y 'prompt_imagen' (que contendrá un prompt descriptivo en inglés, de no más de 100 palabras, para que un generador de imágenes como DALL-E cree una ilustración fotorrealista y profesional que acompañe al texto).\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Analiza el siguiente informe técnico y genera el contenido JSON solicitado:\\n\\n---\\nINFORME TÉCNICO:\\n{extracted_text_pdfplumber}\\n---\"}\n",
        "    ]\n",
        "\n",
        "    model_name = \"gpt-4o\" # gpt-4o es ideal para esta tarea por su calidad y capacidad para seguir instrucciones JSON\n",
        "    print(f\"\\n🤖 Enviando solicitud a OpenAI con el modelo {model_name}. Esto puede tomar unos segundos...\")\n",
        "\n",
        "    try:\n",
        "        # LLAMADA A GPT-4o Y PROCESAMIENTO JSON\n",
        "        response_gpt = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=prompt_messages,\n",
        "            temperature=0.5,\n",
        "            response_format={\"type\": \"json_object\"} # Forzamos una respuesta JSON para mayor fiabilidad\n",
        "        )\n",
        "\n",
        "        # Parseamos la respuesta JSON\n",
        "        raw_content = response_gpt.choices[0].message.content\n",
        "        resultados_json = json.loads(raw_content)\n",
        "\n",
        "        texto_para_linkedin = resultados_json.get(\"texto_linkedin\", \"Error: No se encontró el texto para LinkedIn en la respuesta.\")\n",
        "        prompt_para_dalle = resultados_json.get(\"prompt_imagen\", \"A photorealistic image of the Argentinian automotive market, focusing on new cars in a dealership.\")\n",
        "\n",
        "        print(\"\\n--- ✅ Contenido para LinkedIn generado ---\")\n",
        "        print(texto_para_linkedin)\n",
        "        print(\"\\n--- ✅ Prompt para DALL-E generado ---\")\n",
        "        print(prompt_para_dalle)\n",
        "\n",
        "        # GUARDAR EL TEXTO EN GOOGLE DRIVE\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        ruta_texto = '/content/drive/MyDrive/post_linkedin.txt'\n",
        "\n",
        "        with open(ruta_texto, 'w', encoding='utf-8') as f:\n",
        "            f.write(texto_para_linkedin)\n",
        "        print(f\"\\n📝 Texto guardado exitosamente en Google Drive: {ruta_texto}\")\n",
        "\n",
        "        # GENERAMOS LA IMAGEN CON DALL-E 3\n",
        "        print(\"\\n🎨 Generando imagen con DALL-E 3. Esto puede tardar un momento...\")\n",
        "        response_dalle = client.images.generate(\n",
        "            model=\"dall-e-3\",\n",
        "            prompt=prompt_para_dalle,\n",
        "            size=\"1024x1024\",\n",
        "            quality=\"standard\",\n",
        "            n=1,\n",
        "        )\n",
        "        image_url = response_dalle.data[0].url\n",
        "\n",
        "        # GUARDAR Y MOSTRAR LA IMAGEN\n",
        "        ruta_imagen = '/content/drive/MyDrive/imagen_linkedin.png'\n",
        "        response_img = requests.get(image_url)\n",
        "        img = Image.open(BytesIO(response_img.content))\n",
        "\n",
        "        img.save(ruta_imagen)\n",
        "        print(f\"🖼️ Imagen guardada exitosamente en Google Drive: {ruta_imagen}\")\n",
        "\n",
        "        print(\"\\n--- Mostrando imagen generada ---\")\n",
        "        display(img)\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"\\n❌ Error Crítico: La respuesta de OpenAI no fue un JSON válido. No se puede continuar.\")\n",
        "        print(\"Respuesta recibida:\", raw_content)\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Ocurrió un error durante la generación o guardado: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n--- Proceso detenido: No se pudo extraer texto del PDF para generar el prompt. ---\")"
      ],
      "metadata": {
        "id": "MTq7rKqqx7Jr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}