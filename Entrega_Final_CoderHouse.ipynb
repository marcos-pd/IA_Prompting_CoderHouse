{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNZL5vTcjELvy3RJ9wWw6i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcos-pd/IA_Prompting_CoderHouse/blob/main/Entrega_Final_CoderHouse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generaci√≥n de contenidos de difusi√≥n a partir de informes t√©cnicos**\n",
        "*Marcos Ferrrario*"
      ],
      "metadata": {
        "id": "HgfKpUh6basZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*La soluci√≥n que vamos a construir permite cargar un informe en pdf y mediante herramientas y librer√≠as en Python extraer el texto que contiene. Ese texto se guardar√° en en un archivo JSON y se pasar√° a un modelo LLM para que, mediante un prompt optimizado, el modelo devuelva un texto con engagement que pueda usarse para su difusi√≥n mediante RS, email y publicaci√≥n en web institucional.\n",
        "\n",
        "La soluci√≥n se optimiza haciendo que el mismo resultado del LLM, genere un prompt coherente para pasarlo a Dall-E y que √©ste genere una im√°gen para ilustrar la publicaci√≥n.*\n"
      ],
      "metadata": {
        "id": "Fcdy_mA-b_7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *EXTRACI√ìN DE TEXTO desde PDF*"
      ],
      "metadata": {
        "id": "LPocFOJmcvN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para la extracci√≥n de PDFs, las bibliotecas m√°s comunes y robustas en Python son PyPDF2 (para PDFs basados en texto) y pdfplumber (que maneja mejor la estructura y tablas, y es m√°s potente).\n",
        "\n",
        "No es nuestro caso, pero para resolver el problema de pdfs escaneados, hay que usar una herramienta de OCR, donde PyTesseract es una buena opci√≥n."
      ],
      "metadata": {
        "id": "MASfBeCD3jfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar bibliotecas necesarias (ejecutar solo una vez por sesi√≥n)\n",
        "!pip install PyPDF2\n",
        "!pip install pdfplumber\n",
        "!pip install google-colab\n",
        "\n",
        "from google.colab import files\n",
        "import pdfplumber\n",
        "import re # Para futuras extracciones num√©ricas"
      ],
      "metadata": {
        "id": "ev5GZpxiEKDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subir el archivo PDF\n",
        "print(\"Por favor, selecciona y sube tu archivo PDF (solo uno):\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Validamos la subida del archivo. Funciona mentras subas un solo archivo.\n",
        "if uploaded:\n",
        "    pdf_file_path = list(uploaded.keys())[0]\n",
        "    print(f\"Archivo '{pdf_file_path}' subido exitosamente.\")\n",
        "else:\n",
        "    pdf_file_path = None\n",
        "    print(\"No se ha subido ning√∫n archivo.\")"
      ],
      "metadata": {
        "id": "yZpgjBXaekWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraemos el texto del PDF\n",
        "def extract_info_from_pdf_pdfplumber(pdf_path):\n",
        "    full_text = \"\"\n",
        "    extracted_tables = []\n",
        "\n",
        "    if not pdf_path:\n",
        "        print(\"Error: No se proporcion√≥ una ruta de archivo PDF v√°lida.\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                # Extraer texto de la p√°gina\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    full_text += page_text + \"\\n\\n\"\n",
        "\n",
        "                # Extraer tablas de la p√°gina\n",
        "                tables = page.extract_tables()\n",
        "                if tables:\n",
        "                    extracted_tables.extend(tables)\n",
        "    except Exception as e:\n",
        "        print(f\"Error al procesar el PDF '{pdf_path}': {e}\")\n",
        "        return None, None\n",
        "\n",
        "    return full_text, extracted_tables"
      ],
      "metadata": {
        "id": "tcgPOZm9EMU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando la extracci√≥n\n",
        "if 'pdf_file_path' in locals() and pdf_file_path:\n",
        "    print(f\"\\nIntentando extraer informaci√≥n de: {pdf_file_path}\")\n",
        "    extracted_text_pdfplumber, extracted_tables_pdfplumber = extract_info_from_pdf_pdfplumber(pdf_file_path)\n",
        "\n",
        "    if extracted_text_pdfplumber is not None:\n",
        "        print(\"\\n--- Texto extra√≠do con pdfplumber (primeros 1000 caracteres) ---\")\n",
        "        print(extracted_text_pdfplumber[:1000])\n",
        "        print(\"\\n--- Fin del fragmento de texto ---\")\n",
        "\n",
        "        if len(extracted_text_pdfplumber) > 1000:\n",
        "            print(f\"\\n(El texto completo tiene {len(extracted_text_pdfplumber)} caracteres. Se mostraron los primeros 1000.)\")\n",
        "\n",
        "        if extracted_tables_pdfplumber:\n",
        "            print(f\"\\n--- Se encontraron {len(extracted_tables_pdfplumber)} tabla(s) ---\")\n",
        "            for i, table in enumerate(extracted_tables_pdfplumber):\n",
        "                print(f\"\\nTabla {i+1} (primeras 3 filas):\")\n",
        "                for row_idx, row in enumerate(table):\n",
        "                    print(row)\n",
        "                    if row_idx >= 2:\n",
        "                        break\n",
        "                if i >= 0: # Para mostrar solo la primera tabla.\n",
        "                    break\n",
        "        else:\n",
        "            print(\"\\n--- No se encontraron tablas con pdfplumber. ---\")\n",
        "    else:\n",
        "        print(\"\\n--- La extracci√≥n de PDF fall√≥ o no se pudo obtener texto/tablas. ---\")\n",
        "else:\n",
        "    print(\"\\nError: La ruta del archivo PDF no est√° disponible. Asegurate de haber subido el archivo  y de que la variable 'pdf_file_path' se haya creado correctamente.\")"
      ],
      "metadata": {
        "id": "HrsqiEFwEOpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *PROCESAMIENTO DEL TEXTO EXTRAIDO a trav√©s del modelo OpenAI gpt 4o*"
      ],
      "metadata": {
        "id": "YNtGsAQUlILN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import json\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Carga la clave de API desde \"los secretos\" de Colab\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Inicializa el cliente de OpenAI\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Prueba una peque√±a solicitud para verificar la conexi√≥n\n",
        "try:\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\", #\"gpt-3.5-turbo\"\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Hola, ¬øest√°s funcionando?\"}]\n",
        "    )\n",
        "    print(\"Conexi√≥n a OpenAI exitosa:\", response.choices[0].message.content)\n",
        "except Exception as e:\n",
        "    print(f\"Error al conectar con OpenAI: {e}\")\n",
        "    print(\"Aseg√∫rate de que tu clave de API est√© configurada correctamente en los secretos de Colab y que tengas saldo disponible.\")"
      ],
      "metadata": {
        "id": "xVLeqxDtHjt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora s√≠. Vamos a pasarle a ChatGPT un prompt **usando la t√©cnica de One Shot Prompting** para darle contexto y especificarle al modelo que genere un resultado que optimice el uso de recursos y minimice los costos.\n",
        "\n",
        "Este prompt le pedir√° al sistema que genere un texto para publicar y un prompt para que Dall-E genere una im√°gen ilustrativa para la publicaci√≥n, por ejemplo, en Linkedin.\n",
        "\n",
        "**Este m√©todo nos permite ahorrar costos ya que realizamos una sola llamada a la API en lugar de dos y aumentar la coherencia, ya que el mismo modelo que entendi√≥ y resumi√≥ el texto es el que sugiere la imagen, asegurando que el concepto visual est√© perfectamente alineado con el textual**.\n",
        "\n",
        "El prompt le pedir√° a la API que genere un JSON con dos claves: una para el texto y otra para el prompt de la imagen.\n",
        "\n",
        "Luego usar√° esas dos salidas para generar un archivo .txt con los textos para la publicaci√≥n, que se va a guardar en una carpeta en Google Drive y llamar√° a Dall-E para generar la im√°gen a partir de la segunda parte de la salida."
      ],
      "metadata": {
        "id": "3muSRrcXWA3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'extracted_text_pdfplumber' in locals() and extracted_text_pdfplumber:\n",
        "    prompt_messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Eres economista con muy buen conocimento de la macroeconom√≠a argentina y un fuerte expertice en el mercado automotor. Tu diferencial como analista es tu capacidad para entender las implicancias de las novedades macroecon√≥micas en el negocio automotriz, desde la industria (f√°bricas, autopartistas o importadores) hasta el retail minorista (la venta de veh√≠culos nuevos y usados al consumidor final). En tu rol, tienes el objetivo de analizar los informes t√©cnicos que se te proveeran y a partir de ellos generar contenido para los canales de difusi√≥n de la empresa que los promueve. Ese contenido est√° orientado a Redes Sociales como Linkedin y a textos con engadgement para difundir los informes por email o en el sitio web de la empresa. Para generar esos contenidos, es importante que mantengas ciertas cualidades del lenguaje humano a trav√©s de las siguientes premisas: 1) Manteniendo la estructura general del texto y el rigor t√©cnico, agregando emociones sutiles, expresiones humanas y lenguaje conversacional. 2) Redactar el texto con frases de ritmo variado, mezclando oraciones cortas con oraciones largas. 3) Sustituye palabras gen√©ricas por otras m√°s espec√≠ficas que contengan carga emocional. 4) Agrega conectores suaves y naturales como ‚Äúadem√°s‚Äù, ‚Äúy es que‚Äù, etc. En la medida de lo posible usa ejemplos, analog√≠as o detalles que hagan m√°s cercano el texto al lector. 5) Evita estructuras gramaticales repetitivas, frases sim√©tricas o lenguaje neutro. Finalmente, estructura tu respuesta √∫nicamente como un objeto JSON v√°lido con las siguientes dos claves: 'texto_linkedin' (que contendr√° el texto generado para la publicaci√≥n) y 'prompt_imagen' (que contendr√° un prompt descriptivo en ingl√©s, de no m√°s de 100 palabras, para que un generador de im√°genes como DALL-E cree una ilustraci√≥n fotorrealista y profesional que acompa√±e al texto).\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Analiza el siguiente informe t√©cnico y genera el contenido JSON solicitado:\\n\\n---\\nINFORME T√âCNICO:\\n{extracted_text_pdfplumber}\\n---\"}\n",
        "    ]\n",
        "\n",
        "    model_name = \"gpt-4o\" # gpt-4o es ideal para esta tarea por su calidad y capacidad para seguir instrucciones JSON\n",
        "    print(f\"\\nü§ñ Enviando solicitud a OpenAI con el modelo {model_name}. Esto puede tomar unos segundos...\")\n",
        "\n",
        "    try:\n",
        "        # LLAMADA A GPT-4o Y PROCESAMIENTO JSON\n",
        "        response_gpt = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=prompt_messages,\n",
        "            temperature=0.5,\n",
        "            response_format={\"type\": \"json_object\"} # Forzamos una respuesta JSON para mayor fiabilidad\n",
        "        )\n",
        "\n",
        "        # Parseamos la respuesta JSON\n",
        "        raw_content = response_gpt.choices[0].message.content\n",
        "        resultados_json = json.loads(raw_content)\n",
        "\n",
        "        texto_para_linkedin = resultados_json.get(\"texto_linkedin\", \"Error: No se encontr√≥ el texto para LinkedIn en la respuesta.\")\n",
        "        prompt_para_dalle = resultados_json.get(\"prompt_imagen\", \"A photorealistic image of the Argentinian automotive market, focusing on new cars in a dealership.\")\n",
        "\n",
        "        print(\"\\n--- ‚úÖ Contenido para LinkedIn generado ---\")\n",
        "        print(texto_para_linkedin)\n",
        "        print(\"\\n--- ‚úÖ Prompt para DALL-E generado ---\")\n",
        "        print(prompt_para_dalle)\n",
        "\n",
        "        # GUARDAR EL TEXTO EN GOOGLE DRIVE\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        ruta_texto = '/content/drive/MyDrive/post_linkedin.txt'\n",
        "\n",
        "        with open(ruta_texto, 'w', encoding='utf-8') as f:\n",
        "            f.write(texto_para_linkedin)\n",
        "        print(f\"\\nüìù Texto guardado exitosamente en Google Drive: {ruta_texto}\")\n",
        "\n",
        "        # GENERAMOS LA IMAGEN CON DALL-E 3\n",
        "        print(\"\\nüé® Generando imagen con DALL-E 3. Esto puede tardar un momento...\")\n",
        "        response_dalle = client.images.generate(\n",
        "            model=\"dall-e-3\",\n",
        "            prompt=prompt_para_dalle,\n",
        "            size=\"1024x1024\",\n",
        "            quality=\"standard\",\n",
        "            n=1,\n",
        "        )\n",
        "        image_url = response_dalle.data[0].url\n",
        "\n",
        "        # GUARDAR Y MOSTRAR LA IMAGEN\n",
        "        ruta_imagen = '/content/drive/MyDrive/imagen_linkedin.png'\n",
        "        response_img = requests.get(image_url)\n",
        "        img = Image.open(BytesIO(response_img.content))\n",
        "\n",
        "        img.save(ruta_imagen)\n",
        "        print(f\"üñºÔ∏è Imagen guardada exitosamente en Google Drive: {ruta_imagen}\")\n",
        "\n",
        "        print(\"\\n--- Mostrando imagen generada ---\")\n",
        "        display(img)\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"\\n‚ùå Error Cr√≠tico: La respuesta de OpenAI no fue un JSON v√°lido. No se puede continuar.\")\n",
        "        print(\"Respuesta recibida:\", raw_content)\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Ocurri√≥ un error durante la generaci√≥n o guardado: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n--- Proceso detenido: No se pudo extraer texto del PDF para generar el prompt. ---\")"
      ],
      "metadata": {
        "id": "MTq7rKqqx7Jr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}